{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drickkarmokar5776/-Classify-Vibration-Data/blob/main/vibration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775OsmvJ2IDA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lceAFzOI1Z9s"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Import numpy\n",
        "import pandas as pd  # Import pandas\n",
        "\n",
        "# Assuming data is loaded as a NumPy array from a file, like so:\n",
        "# other_data = np.load('other_data.npy')\n",
        "\n",
        "# Sample Float64 data (replace this with actual loading code)\n",
        "other_data = np.random.randn(1000, 10)  # 1000 samples, 10 features\n",
        "\n",
        "# Convert to DataFrame\n",
        "other_df = pd.DataFrame(other_data, columns=[f'Feature_{i}' for i in range(other_data.shape[1])])\n",
        "\n",
        "# Add labels (replace with actual labels)\n",
        "other_df['Label'] = np.random.randint(0, 12, size=len(other_df))\n",
        "\n",
        "# Save to CSV\n",
        "other_df.to_csv('other_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJO_BNnF1fH5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example for loading Complex 128 data\n",
        "# Assuming data is loaded as a NumPy array from a file, like so:\n",
        "# S1_data = np.load('S1_data.npy')\n",
        "\n",
        "# Sample Complex128 data (replace this with actual loading code)\n",
        "S1_data = np.random.randn(1000) + 1j * np.random.randn(1000)\n",
        "\n",
        "# Convert to DataFrame with separate columns for real and imaginary parts\n",
        "S1_df = pd.DataFrame({\n",
        "    'Real_Part': S1_data.real,\n",
        "    'Imaginary_Part': S1_data.imag\n",
        "})\n",
        "\n",
        "# Add labels (replace with actual labels)\n",
        "# Assuming we have corresponding labels for each data point\n",
        "S1_df['Label'] = np.random.randint(0, 12, size=len(S1_df))\n",
        "\n",
        "# Save to CSV\n",
        "S1_df.to_csv('S1_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGEopZWp6O5O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load all the .dat files\n",
        "data1 = np.fromfile(\"/content/S10_train_10msps_1sec.dat\")\n",
        "data2 = np.fromfile(\"/content/S11_train_10msps_1sec.dat\")\n",
        "data3 = np.fromfile(\"/content/S2_train_10msps_1sec.dat\")\n",
        "data4 = np.fromfile(\"/content/S3_train_10msps_1sec.dat\")\n",
        "data5 = np.fromfile(\"/content/S4_train_10msps_1sec.dat\")\n",
        "data6 = np.fromfile(\"/content/S5_train_10msps_1sec.dat\")\n",
        "data7 = np.fromfile(\"/content/S6_train_10msps_1sec.dat\")\n",
        "data8 = np.fromfile(\"/content/S7_train_10msps_1sec.dat\")\n",
        "data9 = np.fromfile(\"/content/S8_data.dat\")\n",
        "data10 = np.fromfile(\"/content/S1_Data.dat\")\n",
        "data11 = np.fromfile(\"/content/S12_Data.dat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEt-0NLf6Clo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert the numpy arrays to DataFrames\n",
        "df1 = pd.DataFrame(data1, columns=[\"S10_Vibration\"])\n",
        "df2 = pd.DataFrame(data2, columns=[\"S11_Vibration\"])\n",
        "df3 = pd.DataFrame(data3, columns=[\"S2_Vibration\"])\n",
        "df4 = pd.DataFrame(data4, columns=[\"S3_Vibration\"])\n",
        "df5 = pd.DataFrame(data5, columns=[\"S4_Vibration\"])\n",
        "df6 = pd.DataFrame(data6, columns=[\"S5_Vibration\"])\n",
        "df7 = pd.DataFrame(data7, columns=[\"S6_Vibration\"])\n",
        "df8 = pd.DataFrame(data8, columns=[\"S7_Vibration\"])\n",
        "df9 = pd.DataFrame(data9, columns=[\"S8_Vibration\"])\n",
        "df10 = pd.DataFrame(data10, columns=[\"S1_Vibration\"])\n",
        "df11 = pd.DataFrame(data11, columns=[\"S12_Vibration\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bryPDtyH_eTg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply normalization to all DataFrames\n",
        "df1_scaled = pd.DataFrame(scaler.fit_transform(df1), columns=df1.columns)\n",
        "df2_scaled = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)\n",
        "df3_scaled = pd.DataFrame(scaler.fit_transform(df3), columns=df3.columns)\n",
        "df4_scaled = pd.DataFrame(scaler.fit_transform(df4), columns=df4.columns)\n",
        "df5_scaled = pd.DataFrame(scaler.fit_transform(df5), columns=df5.columns)\n",
        "df6_scaled = pd.DataFrame(scaler.fit_transform(df6), columns=df6.columns)\n",
        "df7_scaled = pd.DataFrame(scaler.fit_transform(df7), columns=df7.columns)\n",
        "df8_scaled = pd.DataFrame(scaler.fit_transform(df8), columns=df8.columns)\n",
        "df9_scaled = pd.DataFrame(scaler.fit_transform(df9), columns=df9.columns)\n",
        "df10_scaled = pd.DataFrame(scaler.fit_transform(df10), columns=df10.columns)\n",
        "df11_scaled = pd.DataFrame(scaler.fit_transform(df11), columns=df11.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V_8VC08_gWa"
      },
      "outputs": [],
      "source": [
        "# Save the scaled datasets to CSV files\n",
        "df1_scaled.to_csv('/content/S10_scaled.csv', index=False)\n",
        "df2_scaled.to_csv('/content/S11_scaled.csv', index=False)\n",
        "df3_scaled.to_csv('/content/S2_scaled.csv', index=False)\n",
        "df4_scaled.to_csv('/content/S3_scaled.csv', index=False)\n",
        "df5_scaled.to_csv('/content/S4_scaled.csv', index=False)\n",
        "df6_scaled.to_csv('/content/S5_scaled.csv', index=False)\n",
        "df7_scaled.to_csv('/content/S6_scaled.csv', index=False)\n",
        "df8_scaled.to_csv('/content/S7_scaled.csv', index=False)\n",
        "df9_scaled.to_csv('/content/S8_scaled.csv', index=False)\n",
        "df10_scaled.to_csv('/content/S1_scaled.csv', index=False)\n",
        "df11_scaled.to_csv('/content/S12_scaled.csv', index=False)\n",
        "# Combine all scaled DataFrames into one (if desired)\n",
        "combined_df = pd.concat([df1_scaled, df2_scaled, df3_scaled, df4_scaled, df5_scaled,\n",
        "                         df6_scaled, df7_scaled, df8_scaled, df9_scaled, df10_scaled,\n",
        "                         df11_scaled], axis=0)\n",
        "\n",
        "# Save the combined data\n",
        "combined_df.to_csv('/content/combined_scaled_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye0m-4e6AoqD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "combined_df = pd.read_csv('/content/combined_scaled_data.csv')\n",
        "\n",
        "# Print the contents of the DataFrame\n",
        "print(combined_df)\n",
        "# Print the first few rows of the DataFrame\n",
        "print(combined_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NOOaerGON27"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your data from a CSV file\n",
        "data = pd.read_csv('/content/combined_scaled_data.csv')\n",
        "\n",
        "# Drop columns that are entirely NaN\n",
        "data = data.dropna(axis=1, how='all')\n",
        "\n",
        "# Handle missing values by imputation (filling NaN with mean)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# Standardize the features (PCA is sensitive to the scale of the data)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data_imputed)\n",
        "\n",
        "# Determine the number of components to keep\n",
        "n_components = min(X_scaled.shape[1], 2)  # Set to 2 or less, based on available features\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Optional: Create a DataFrame with the principal components\n",
        "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
        "X_pca_df = pd.DataFrame(X_pca, columns=pca_columns)\n",
        "\n",
        "# Display the explained variance ratio of the principal components\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio of each component:\", explained_variance)\n",
        "\n",
        "# Display the resulting principal components\n",
        "print(X_pca_df.head())\n",
        "\n",
        "# If you want to save the PCA result to a CSV file:\n",
        "X_pca_df.to_csv('pca_output.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNmYAEPd56Pp"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "# Load the PCA-transformed data\n",
        "X_pca_df = pd.read_csv('/content/pca_output.csv')\n",
        "\n",
        "# Perform k-Means clustering to create pseudo-labels\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)  # Assuming 2 clusters; adjust as needed\n",
        "pseudo_labels = kmeans.fit_predict(X_pca_df)\n",
        "\n",
        "# Add the pseudo-labels to the PCA DataFrame\n",
        "X_pca_df['Cluster'] = pseudo_labels\n",
        "\n",
        "# Optional: Display the first few rows of the DataFrame with the new cluster labels\n",
        "print(X_pca_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AaSsPPD9-wTi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the PCA-transformed data\n",
        "X_pca_df = pd.read_csv('/content/pca_output.csv')\n",
        "\n",
        "# Perform k-Means clustering to create pseudo-labels\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)  # Assuming 2 clusters; adjust as needed\n",
        "pseudo_labels = kmeans.fit_predict(X_pca_df)\n",
        "\n",
        "# Add the pseudo-labels to the PCA DataFrame\n",
        "X_pca_df['Cluster'] = pseudo_labels\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca_df.drop(columns=['Cluster']), X_pca_df['Cluster'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the k-NN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "print(\"k-NN Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP7GLCBe_yga"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca_df.drop(columns=['Cluster']), X_pca_df['Cluster'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the k-NN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "print(\"k-NN Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your PCA-transformed and clustered data\n",
        "X_pca_df = pd.read_csv('/content/pca_output.csv')\n",
        "\n",
        "# Assuming 'Cluster' is the target for SVM classification\n",
        "X = X_pca_df.drop(columns=['PS1'])\n",
        "y = X_pca_df['Cluster']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# 1 Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "#.2 Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# 3 Classification Report\n",
        "class_report = classification_report(y_test, y_pred_svm)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "5dVIqDhEmbLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZRFqkEfyZF34C9v1OuGRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}